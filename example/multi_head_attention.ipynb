{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate multi head Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 3, 2]), torch.Size([2, 4, 3, 2]), torch.Size([2, 4, 3, 2]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, head, seq_len, d_k = 2, 4, 3, 2\n",
    "shape = (batch_size, head, seq_len, d_k)\n",
    "q = torch.randn(shape)\n",
    "k = torch.randn(shape)\n",
    "v = torch.randn(shape)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = head * d_k\n",
    "d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4516, -0.7912],\n",
       "         [ 0.6224, -0.6406],\n",
       "         [ 0.9762, -0.2074]],\n",
       "\n",
       "        [[-0.4497,  0.0510],\n",
       "         [-0.4176,  0.2446],\n",
       "         [ 0.9037,  1.4893]],\n",
       "\n",
       "        [[ 0.2951,  0.4999],\n",
       "         [-0.5982,  1.5882],\n",
       "         [-0.3139, -2.0337]],\n",
       "\n",
       "        [[ 0.4014,  0.5351],\n",
       "         [ 0.4024,  0.2968],\n",
       "         [ 0.4885,  0.8046]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4516, -0.7912,  0.6224, -0.6406,  0.9762, -0.2074, -0.4497,\n",
       "           0.0510],\n",
       "         [-0.4176,  0.2446,  0.9037,  1.4893,  0.2951,  0.4999, -0.5982,\n",
       "           1.5882],\n",
       "         [-0.3139, -2.0337,  0.4014,  0.5351,  0.4024,  0.2968,  0.4885,\n",
       "           0.8046]],\n",
       "\n",
       "        [[-1.7663, -1.0167, -0.4292,  1.1023,  1.2906, -0.8918, -0.4826,\n",
       "           1.1973],\n",
       "         [ 0.3470, -0.4257,  1.1276,  0.3564, -0.0685,  0.6590,  1.5181,\n",
       "          -1.5887],\n",
       "         [-0.2452,  1.7298, -0.7800,  1.6039,  0.5995,  1.1992,  1.0092,\n",
       "           0.1009]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.view(batch_size, -1, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=8, bias=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "linear_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Linear Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 8]), torch.Size([2, 3, 8]), torch.Size([2, 3, 8]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q = linear_layer(q.view(batch_size, -1, d_model))\n",
    "w_k = linear_layer(k.view(batch_size, -1, d_model))\n",
    "w_v = linear_layer(v.view(batch_size, -1, d_model))\n",
    "\n",
    "w_q.shape, w_k.shape, w_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5124, -0.1605,  0.7902, -0.0840,  0.2493, -0.1333, -0.6790,\n",
       "           0.1412],\n",
       "         [ 0.5485,  0.6827, -0.6042,  1.0182,  0.6211, -0.8116, -0.2984,\n",
       "           0.3787],\n",
       "         [ 0.3681,  0.7303, -0.8663, -0.0063,  0.5300, -0.1834, -0.8838,\n",
       "          -0.2012]],\n",
       "\n",
       "        [[ 0.7192,  1.5630, -0.8405,  1.5888,  0.0951, -0.3763, -0.1109,\n",
       "           0.6873],\n",
       "         [-1.1264,  0.1490,  0.2615, -1.0148, -0.3084,  0.6448, -0.9948,\n",
       "          -1.0468],\n",
       "         [-0.3558,  0.9942, -0.1582,  0.8780, -0.4400, -0.0886, -0.1796,\n",
       "          -0.8628]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 3, 2]), torch.Size([2, 4, 3, 2]), torch.Size([2, 4, 3, 2]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q_r = w_q.view(batch_size, -1, head, d_k).transpose(1,2)\n",
    "w_k_r = w_k.view(batch_size, -1, head, d_k).transpose(1,2)\n",
    "w_v_r = w_v.view(batch_size, -1, head, d_k).transpose(1,2)\n",
    "\n",
    "w_q_r.shape, w_k_r.shape, w_v_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5124, -0.1605],\n",
       "         [ 0.5485,  0.6827],\n",
       "         [ 0.3681,  0.7303]],\n",
       "\n",
       "        [[ 0.7902, -0.0840],\n",
       "         [-0.6042,  1.0182],\n",
       "         [-0.8663, -0.0063]],\n",
       "\n",
       "        [[ 0.2493, -0.1333],\n",
       "         [ 0.6211, -0.8116],\n",
       "         [ 0.5300, -0.1834]],\n",
       "\n",
       "        [[-0.6790,  0.1412],\n",
       "         [-0.2984,  0.3787],\n",
       "         [-0.8838, -0.2012]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q_r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 3, 2]), torch.Size([2, 4, 3, 3]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, attention = scaled_dot_product(w_q_r, w_k_r, w_v_r)\n",
    "\n",
    "values.shape, attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9383,  0.6550],\n",
       "         [-0.8682,  0.6854],\n",
       "         [-0.8406,  0.6796]],\n",
       "\n",
       "        [[-0.5437,  0.2594],\n",
       "         [-0.5576,  0.1968],\n",
       "         [-0.3841,  0.1295]],\n",
       "\n",
       "        [[-0.4620,  0.7077],\n",
       "         [-0.2423,  0.5695],\n",
       "         [-0.3570,  0.6486]],\n",
       "\n",
       "        [[-0.1075, -0.6816],\n",
       "         [-0.0758, -0.6241],\n",
       "         [-0.1374, -0.7357]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3172, 0.3968, 0.2861],\n",
       "         [0.2640, 0.4031, 0.3329],\n",
       "         [0.2694, 0.3795, 0.3511]],\n",
       "\n",
       "        [[0.2294, 0.3250, 0.4457],\n",
       "         [0.3024, 0.4633, 0.2343],\n",
       "         [0.4749, 0.3017, 0.2234]],\n",
       "\n",
       "        [[0.2816, 0.3492, 0.3692],\n",
       "         [0.1932, 0.3555, 0.4513],\n",
       "         [0.2342, 0.3642, 0.4016]],\n",
       "\n",
       "        [[0.3537, 0.3123, 0.3340],\n",
       "         [0.2909, 0.4228, 0.2863],\n",
       "         [0.4128, 0.2085, 0.3787]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
